{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经元网络 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经元网络基本发展历程\n",
    "\n",
    "对于神经元的研究由来已久，1904年生物学家就已经知晓了神经元的组成结构。\n",
    "\n",
    "<img src=./img/bio_neuron.png>\n",
    "<img src=./img/neuron_cell_small.png>\n",
    "\n",
    "神经元即神经元细胞，是神经系统最基本的结构和功能单位。分为细胞体和突起两部分。细胞体由细胞核、细胞膜、细胞质组成，具有联络和整合输入信息并传出信息的作用。突起有树突(dendrites)和轴突(axon)两种。树突短而分枝多，直接由细胞体扩张突出，形成树枝状，其作用是接受其他神经元轴突传来的冲动并传给细胞体。轴突长而分枝少，为粗细均匀的细长突起，常起于轴丘，其作用是接受外来刺激，再由细胞体传出。轴突除分出侧枝外，其末端形成树枝样的神经末梢。末梢分布于某些组织器官内，形成各种神经末梢装置。感觉神经末梢形成各种感受器；运动神经末梢分布于骨骼肌肉，形成运动终极。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./img/art_neuron.png>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个神经元的计算模型MpNeuron模型是由Warren MuCulloch(神经科学家)和Walter Pitts(逻辑学家)在1943年提出的。\n",
    "$$\n",
    "\\hat{y} = \\left(\\sum_{i=1}^n x_i\\right) - b > 0\n",
    "$$\n",
    "MpNeuron模型，虽然简单，但已经建立了神经网络大厦的地基。\n",
    "\n",
    "后面为纪念McCulloch和Pitts（1943）的开拓性工作，激活函数为阈值函数的神经元也被称为McCulloch-Pitts模型。 \n",
    "\n",
    "\n",
    "<img src=./img/Warren_Walter.png>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1949年心理学家Hebb提出了Hebb学习率，认为人脑神经细胞的突触（也就是连接）上的强度上可以变化的。于是计算科学家们开始考虑用调整权值的方法来让机器学习。这为后面的学习算法奠定了基础。\n",
    "\n",
    "尽管神经元模型与Hebb学习律都已诞生，但限于当时的计算机能力，直到接近10年后，第一个真正意义的神经网络才诞生。\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1958年，计算科学家Frank Rosenblatt在康奈尔航空实验室（Cornell Aeronautical Laboratory）提出了由两层神经元组成的神经网络，并起了一个名字--“感知器”（Perceptron）。感知器是当时首个可以学习的人工神经网络。\n",
    "\n",
    "<img src=./img/Rosenblatt.png>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./img/one_layer.png>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与神经元模型不同，感知器中的权值是通过训练得到的。因此，根据以前的知识我们知道，感知器类似一个逻辑回归模型，可以做线性分类任务。\n",
    "\n",
    "Marvin Minsky在1969年出版了一本叫《Perceptrons: an introduction to computational geometry》的书，里面用详细的数学证明了感知器的弱点，尤其是感知器对XOR（异或）这样的简单分类任务都无法解决。Minsky认为，如果将计算层增加到两层，计算量则过大，而且没有有效的学习算法。所以，他认为研究更深层的网络是没有价值的。\n",
    "\n",
    "但是事实上对三层感知器的研究表明了是可以实现这些功能。Rosenblatt在他的书中证明了具有先验无限个隐藏层和一个输出神经元的初等感知器可以解决任何分类问题。\n",
    "\n",
    "无论如何，这本书导致了20世纪70年代和80年代初神经网络研究的衰落，这个时期又被称为“AI winter”。大概10年以后，对于两层神经网络的研究才带来神经网络的复苏。\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1986年，Rumelhar和Hinton等人提出了反向传播（Backpropagation，BP）算法，解决了两层神经网络所需要的复杂计算量问题，从而带动了业界使用两层神经网络研究的热潮。目前，大量的教授神经网络的教材，都是重点介绍两层（带一个隐藏层）神经网络的内容。 \n",
    "\n",
    "<img src=./img/two_layer.jpg>\n",
    "\n",
    "与单层神经网络不同。理论证明，两层神经网络可以在一个紧致集上逼近任意连续函数[Universal approximation theorem]。[相关文献](http://neuralnetworksanddeeplearning.com/chap4.html)，直观演示[A visual proof that neural nets can compute any function](http://neuralnetworksanddeeplearning.com/chap4.html)。\n",
    "\n",
    "反向传播算法的启示是数学中的链式法则。在此需要说明的是，尽管早期神经网络的研究人员努力从生物学中得到启发，但从BP算法开始，研究者们更多地从数学上寻求问题的最优解。不再盲目模拟人脑网络是神经网络研究走向成熟的标志。正如科学家们可以从鸟类的飞行中得到启发，但没有必要一定要完全模拟鸟类的飞行方式，也能制造可以飞天的飞机。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两层神经网络在多个地方的应用说明了其效用与价值。10年前困扰神经网络界的异或问题被轻松解决。神经网络在这个时候，已经可以发力于语音识别，图像识别，自动驾驶等多个领域。\n",
    "\n",
    "但是神经网络仍然存在若干的问题：尽管使用了BP算法，一次神经网络的训练仍然耗时太久，而且困扰训练优化的一个问题就是局部最优解问题，这使得神经网络的优化较为困难。同时，隐藏层的节点数需要调参，这使得使用不太方便，工程和研究人员对此多有抱怨。\n",
    "\n",
    "90年代中期，由Vladimir Naumovich Vapnik等人发明的SVM（Support Vector Machines，支持向量机）算法诞生。原始SVM算法是由弗拉基米尔·瓦普尼克和亚历克塞·泽范兰杰斯（ Alexey Ya. Chervonenkis）于1963年发明的。1992年，Bernhard E. Boser、Isabelle M. Guyon和弗拉基米尔·瓦普尼克提出了一种通过将核技巧应用于最大间隔超平面来创建非线性分类器的方法。当前标准的前身（软间隔）由Corinna Cortes和Vapnik于1993年提出，并于1995年发表。\n",
    "\n",
    "SVM对比神经网络的优势：无需调参；高效；全局最优解。基于以上种种理由，SVM迅速打败了神经网络算法成为主流，神经网络的研究再次陷入了冰河期。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2006年，Hinton在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。与传统的训练方式不同，“深度信念网络”有一个“预训练”（pre-training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“微调”(fine-tuning)技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的时间。他给多层神经网络相关的学习方法赋予了一个新名词--“深度学习”。\n",
    "\n",
    "很快，深度学习在语音识别领域暂露头角。接着，2012年，深度学习技术又在图像识别领域大展拳脚。Hinton与他的学生在ImageNet竞赛中，用多层的卷积神经网络成功地对包含一千类别的一百万张图片进行了训练，取得了分类错误率15%的好成绩，这个成绩比第二名高了近11个百分点，充分证明了多层神经网络识别效果的优越性。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前，深度神经网络在人工智能界占据统治地位。但凡有关人工智能的产业报道，必然离不开深度学习。神经网络界当下的四位引领者除了前文所说的Ng，Hinton以外，还有CNN的发明人Yann Lecun，以及《Deep Learning》的作者Bengio。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./img/nn_story.jpg>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
